# Платформа больших данных РЖД Медицина



## Цели и задачи проекта

- создание единой платформы больших данных для отчетности и аналитики РЖД Медицины;
- обеспечение автоматизации процессов загрузки и обработки поступающих данных;
- предоставление пользователю инструментов для анализа и визуализации качества данных;
- обеспечение безопасности и конфиденциальности информации в соответствии с требованиями законодательства.

## Реализуемые возможности

- автоматическая загрузка данных из excel фалов загружаемых пользователя в minio s3 и object storage облачного хранилища VK по средствам ui интерфейса LuxmsBI;
- автоматическая загрузка данных поступающих из шины 1С в Apache Kafka и трансформация;
- трансформация поступающих данных и распределение по слоям;
- построение витрин данных.

## Технологический стек

### Оркестрация и планирование
- **Apache Airflow** - оркестрация ETL/ELT процессов, мониторинг и управление пайплайнами данных

### Обработка данных
- **Apache Flink** - потоковая обработка данных из Apache Kafka
- **dbt (data build tool)** - трансформация данных и построение витрин данных
- **StarRocks** - аналитическая СУБД для хранения и обработки больших объемов данных

### Хранилища данных
- **MinIO** - S3-совместимое объектное хранилище для файлов Excel
- **VK Cloud Object Storage** - облачное хранилище для финансовых форм
- **Apache Kafka** - потоковая платформа для приема данных из шины 1С
- **PostgreSQL** - база данных для метаданных и служебной информации

### Инфраструктура и деплой
- **Ansible** - автоматизация развертывания и конфигурации компонентов
- **LuxmsBI** - UI интерфейс для загрузки файлов пользователями

## Структура проекта

```
rzdm-data-platform/
├── airflow/                    # Apache Airflow компоненты
│   ├── dags/                   # DAG'и для оркестрации процессов
│   │   ├── sandbox_load_from_excel_to_starrocks.py
│   │   ├── main_load_from_excel_to_starrocks.py
│   │   ├── forms_load_excel_to_starrocks.py
│   │   └── ...                 # Другие DAG'и для различных процессов
│   └── plugins/                # Кастомные операторы и хуки
│       ├── base_operators/     # Базовые операторы и миксины
│       ├── dbt_operators/      # Операторы для работы с dbt
│       ├── minio_operators/    # Операторы для работы с MinIO
│       ├── starrocks_operators/# Операторы для работы со StarRocks
│       └── vk_cloud_operators/ # Операторы для работы с VK Cloud
│
├── dbt/                        # Проекты dbt для трансформации данных
│   ├── main_dbt_project/       # Основной проект dbt
│   │   ├── models/
│   │   │   ├── staging/        # Слой staging - сырые данные
│   │   │   ├── rzdm_rdv/       # Слой RDV (Raw Data Vault)
│   │   │   ├── rzdm_bdv/       # Слой BDV (Business Data Vault)
│   │   │   ├── rzdm_mart/      # Витрины данных (dims и facts)
│   │   │   └── rzdm_report/    # Отчетные витрины
│   │   └── macros/             # Макросы для переиспользования логики
│   ├── financial_dbt_project/  # Проект для финансовых форм
│   └── sandbox_dbt_project/    # Проект для sandbox окружения
│
├── flink/                      # Apache Flink приложения
│   ├── src/main/java/         # Java код для потоковой обработки
│   └── pom.xml                 # Maven конфигурация
│
├── deploy/                     # Скрипты и конфигурации для деплоя
│   ├── ansible/                # Ansible playbooks и роли
│   │   ├── roles/              # Роли для развертывания компонентов
│   │   │   ├── airflow-dags/
│   │   │   ├── airflow-plugins/
│   │   │   ├── dbt-projects/
│   │   │   ├── flink-jobs/
│   │   │   └── ...
│   │   └── deploy.yml          # Основной playbook для деплоя
│   └── scripts/                # Вспомогательные скрипты
│       └── deploy.sh           # Скрипт для запуска деплоя
│
└── demo_message/               # Примеры сообщений для тестирования
```

### Описание компонентов

#### Airflow DAGs
- **sandbox_load_from_excel_to_starrocks.py** - загрузка Excel файлов из MinIO в StarRocks (sandbox окружение)
- **main_load_from_excel_to_starrocks.py** - основная загрузка Excel файлов в StarRocks
- **forms_load_excel_to_starrocks.py** - загрузка финансовых форм из VK Cloud в StarRocks
- **pmu_revenue_evaluation*.py** - DAG'и для оценки доходов ПМУ

#### Airflow Plugins
- **base_operators/** - базовые операторы для валидации файлов, работы с S3, проверки метаданных
- **dbt_operators/** - операторы для запуска dbt моделей и типизации данных
- **minio_operators/** - операторы для мониторинга и работы с файлами в MinIO
- **starrocks_operators/** - операторы для загрузки данных в StarRocks, управления таблицами
- **vk_cloud_operators/** - операторы для работы с VK Cloud S3

#### dbt Projects
Проекты организованы по принципу многослойной архитектуры данных:
- **staging** - слой для загрузки сырых данных
- **rdv** - Raw Data Vault слой для нормализации данных
- **bdv** - Business Data Vault слой для бизнес-логики
- **mart** - витрины данных (dimensions и facts)
- **report** - отчетные витрины для аналитики

#### Flink
Java приложения для потоковой обработки данных из Apache Kafka с последующей загрузкой в S3 в формате iceberg.

#### Deploy
Ansible playbooks для автоматизированного развертывания всех компонентов платформы на целевых серверах.

## Архитектура данных

Платформа реализует многослойную архитектуру данных:

1. **Слой загрузки (Staging)**
   - Загрузка данных из различных источников (Excel, Kafka)
   - Минимальная валидация и очистка

2. **Слой нормализации (RDV - Raw Data Vault)**
   - Нормализация данных по методологии Data Vault
   - Сохранение истории изменений

3. **Слой бизнес-логики (BDV - Business Data Vault)**
   - Применение бизнес-правил
   - Обогащение данных

4. **Слой витрин данных (Mart)**
   - Dimensions (измерения)
   - Facts (факты)

5. **Слой отчетности (Report)**
   - Готовые витрины для аналитики и отчетности
